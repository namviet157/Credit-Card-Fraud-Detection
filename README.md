# Credit Card Fraud Detection

**Credit Card Fraud Detection** l√† m·ªôt d·ª± √°n ph√°t hi·ªán gian l·∫≠n th·∫ª t√≠n d·ª•ng ƒë∆∞·ª£c x√¢y d·ª±ng ho√†n to√†n b·∫±ng **NumPy**, kh√¥ng s·ª≠ d·ª•ng c√°c th∆∞ vi·ªán Machine Learning c√≥ s·∫µn nh∆∞ scikit-learn hay Pandas. D·ª± √°n n√†y t·∫≠p trung v√†o vi·ªác hi·ªÉu s√¢u c√°c thu·∫≠t to√°n Machine Learning b·∫±ng c√°ch implement t·ª´ ƒë·∫ßu, t·ª´ kh√°m ph√° d·ªØ li·ªáu, ti·ªÅn x·ª≠ l√Ω ƒë·∫øn hu·∫•n luy·ªán m√¥ h√¨nh Logistic Regression.

## üìã M·ª•c l·ª•c

1. [Gi·ªõi thi·ªáu](#1-gi·ªõi-thi·ªáu)
2. [Dataset](#2-dataset)
3. [Method](#3-method)
4. [Installation & Setup](#4-installation--setup)
5. [Usage](#5-usage)
6. [Results](#6-results)
7. [Project Structure](#7-project-structure)
8. [Challenges & Solutions](#8-challenges--solutions)
9. [Future Improvements](#9-future-improvements)
10. [Contributors](#10-contributors)
11. [License](#11-license)

---

## 1. Gi·ªõi thi·ªáu

### 1.1. M√¥ t·∫£ b√†i to√°n

**B√†i to√°n**: Ph√°t hi·ªán gian l·∫≠n trong c√°c giao d·ªãch th·∫ª t√≠n d·ª•ng

- **ƒê·∫ßu v√†o**: Th√¥ng tin v·ªÅ c√°c giao d·ªãch th·∫ª t√≠n d·ª•ng bao g·ªìm:
  - `Time`: Th·ªùi gian giao d·ªãch (t√≠nh b·∫±ng gi√¢y t·ª´ giao d·ªãch ƒë·∫ßu ti√™n)
  - `V1-V28`: 28 features ƒë√£ ƒë∆∞·ª£c PCA transform (·∫©n danh ƒë·ªÉ b·∫£o m·∫≠t)
  - `Amount`: S·ªë ti·ªÅn giao d·ªãch
  
- **ƒê·∫ßu ra**: D·ª± ƒëo√°n giao d·ªãch c√≥ ph·∫£i l√† gian l·∫≠n hay kh√¥ng
  - `0`: Giao d·ªãch b√¨nh th∆∞·ªùng (Normal)
  - `1`: Giao d·ªãch gian l·∫≠n (Fraud)

- **Lo·∫°i b√†i to√°n**: Binary Classification v·ªõi **class imbalance nghi√™m tr·ªçng**
  - T·ª∑ l·ªá gian l·∫≠n ch·ªâ chi·∫øm **0.17%** t·ªïng s·ªë giao d·ªãch
  - ƒê√¢y l√† m·ªôt trong nh·ªØng th√°ch th·ª©c l·ªõn nh·∫•t c·ªßa b√†i to√°n

### 1.2. ƒê·ªông l·ª±c v√† ·ª©ng d·ª•ng th·ª±c t·∫ø

Fraud detection l√† m·ªôt v·∫•n ƒë·ªÅ c·ª±c k·ª≥ quan tr·ªçng trong ng√†nh t√†i ch√≠nh v√† ng√¢n h√†ng:

1. **T·ªïn th·∫•t t√†i ch√≠nh**: 
   - C√°c giao d·ªãch gian l·∫≠n g√¢y thi·ªát h·∫°i h√†ng t·ª∑ USD m·ªói nƒÉm tr√™n to√†n th·∫ø gi·ªõi
   - M·ªói giao d·ªãch gian l·∫≠n kh√¥ng ƒë∆∞·ª£c ph√°t hi·ªán ƒë·ªÅu g√¢y thi·ªát h·∫°i tr·ª±c ti·∫øp

2. **B·∫£o v·ªá kh√°ch h√†ng**:
   - Ph√°t hi·ªán s·ªõm gi√∫p b·∫£o v·ªá kh√°ch h√†ng kh·ªèi c√°c ho·∫°t ƒë·ªông gian l·∫≠n
   - Gi·∫£m thi·ªÉu r·ªßi ro m·∫•t ti·ªÅn v√† th√¥ng tin c√° nh√¢n

3. **Tu√¢n th·ªß quy ƒë·ªãnh**:
   - C√°c ng√¢n h√†ng v√† t·ªï ch·ª©c t√†i ch√≠nh c·∫ßn c√≥ h·ªá th·ªëng ph√°t hi·ªán gian l·∫≠n hi·ªáu qu·∫£ ƒë·ªÉ tu√¢n th·ªß c√°c quy ƒë·ªãnh ph√°p l√Ω

4. **X·ª≠ l√Ω real-time**:
   - C·∫ßn ph√°t hi·ªán gian l·∫≠n trong th·ªùi gian th·ª±c ƒë·ªÉ ngƒÉn ch·∫∑n k·ªãp th·ªùi
   - Y√™u c·∫ßu m√¥ h√¨nh c√≥ ƒë·ªô ch√≠nh x√°c cao v√† t·ªëc ƒë·ªô x·ª≠ l√Ω nhanh

5. **C√¢n b·∫±ng gi·ªØa Precision v√† Recall**:
   - **Precision cao**: Tr√°nh l√†m phi·ªÅn kh√°ch h√†ng b·∫±ng c√°c c·∫£nh b√°o gi·∫£ (False Positives)
   - **Recall cao**: Tr√°nh b·ªè l·ªçt c√°c giao d·ªãch gian l·∫≠n (False Negatives) - ƒëi·ªÅu n√†y c·ª±c k·ª≥ quan tr·ªçng

### 1.3. M·ª•c ti√™u c·ª• th·ªÉ

#### M·ª•c ti√™u k·ªπ thu·∫≠t:
1. **L√†m ch·ªß NumPy**:
   - S·ª≠ d·ª•ng NumPy ƒë·ªÉ x·ª≠ l√Ω to√†n b·ªô d·ªØ li·ªáu (kh√¥ng d√πng Pandas)
   - Implement c√°c thu·∫≠t to√°n ML t·ª´ ƒë·∫ßu b·∫±ng NumPy
   - T·ªëi ∆∞u h√≥a code v·ªõi vectorization v√† broadcasting
   - Tr√°nh s·ª≠ d·ª•ng for loops kh√¥ng c·∫ßn thi·∫øt

2. **Ph√¢n t√≠ch d·ªØ li·ªáu s√¢u**:
   - Kh√°m ph√° v√† hi·ªÉu v·ªÅ dataset
   - Ph√°t hi·ªán patterns v√† insights t·ª´ d·ªØ li·ªáu
   - X·ª≠ l√Ω class imbalance
   - Ph√¢n t√≠ch correlation v√† feature importance

3. **Modeling t·ª´ ƒë·∫ßu**:
   - Implement Logistic Regression ho√†n ch·ªânh v·ªõi Gradient Descent
   - Hi·ªÉu s√¢u v·ªÅ loss function, gradient computation
   - ƒê√°nh gi√° m√¥ h√¨nh v·ªõi c√°c metrics ph√π h·ª£p cho imbalanced data

#### M·ª•c ti√™u h·ªçc thu·∫≠t:
- Hi·ªÉu r√µ c√°ch ho·∫°t ƒë·ªông c·ªßa c√°c thu·∫≠t to√°n ML c∆° b·∫£n
- N·∫Øm v·ªØng c√°c k·ªπ thu·∫≠t x·ª≠ l√Ω d·ªØ li·ªáu
- √Åp d·ª•ng ki·∫øn th·ª©c to√°n h·ªçc v√†o th·ª±c t·∫ø

---

## 2. Dataset

### 2.1. Ngu·ªìn d·ªØ li·ªáu

- **Dataset**: Credit Card Fraud Detection
- **Ngu·ªìn**: [Kaggle - Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
- **T·ªï ch·ª©c**: ULB (Universit√© Libre de Bruxelles) Machine Learning Group
- **K√≠ch th∆∞·ªõc**: 284,807 giao d·ªãch
- **Th·ªùi gian thu th·∫≠p**: Giao d·ªãch trong 2 ng√†y (kho·∫£ng 48 gi·ªù)

### 2.2. M√¥ t·∫£ c√°c features

| Feature | M√¥ t·∫£ | Ki·ªÉu d·ªØ li·ªáu | ƒê·∫∑c ƒëi·ªÉm |
|---------|-------|--------------|----------|
| **Time** | S·ªë gi√¢y gi·ªØa giao d·ªãch ƒë·∫ßu ti√™n v√† giao d·ªãch n√†y | Float | Ph·∫°m vi: 0 - 172,792 gi√¢y |
| **V1-V28** | 28 features ƒë√£ ƒë∆∞·ª£c PCA transform | Float | ƒê√£ ƒë∆∞·ª£c chu·∫©n h√≥a, mean ‚âà 0, std ‚âà 1 |
| **Amount** | S·ªë ti·ªÅn giao d·ªãch (USD) | Float | Ph·∫°m vi: $0 - $25,691.16, ph√¢n ph·ªëi l·ªách ph·∫£i |
| **Class** | Nh√£n (0 = b√¨nh th∆∞·ªùng, 1 = gian l·∫≠n) | Integer | Binary classification target |

**L∆∞u √Ω quan tr·ªçng**: 
- C√°c features V1-V28 ƒë√£ ƒë∆∞·ª£c PCA transform ƒë·ªÉ **b·∫£o m·∫≠t th√¥ng tin nh·∫°y c·∫£m** c·ªßa kh√°ch h√†ng
- ƒê√¢y l√† c√°ch ti·∫øp c·∫≠n ph·ªï bi·∫øn trong c√°c b√†i to√°n t√†i ch√≠nh ƒë·ªÉ tu√¢n th·ªß quy ƒë·ªãnh b·∫£o v·ªá d·ªØ li·ªáu c√° nh√¢n
- C√°c features g·ªëc (nh∆∞ s·ªë th·∫ª, t√™n kh√°ch h√†ng, ƒë·ªãa ch·ªâ) kh√¥ng ƒë∆∞·ª£c ti·∫øt l·ªô

### 2.3. K√≠ch th∆∞·ªõc v√† ƒë·∫∑c ƒëi·ªÉm d·ªØ li·ªáu

#### Th·ªëng k√™ t·ªïng quan:
- **T·ªïng s·ªë samples**: 284,807
- **S·ªë features**: 30 (Time + V1-V28 + Amount)
- **Missing values**: **Kh√¥ng c√≥** (0 missing values)
- **Outliers**: C√≥ nhi·ªÅu outliers, ƒë·∫∑c bi·ªát trong:
  - Feature `Amount`: 31,904 outliers (11.20%) theo IQR method
  - Feature `V27`: 39,163 outliers (13.75%)
  - Feature `V28`: 30,342 outliers (10.65%)

#### Class Distribution (Ph√¢n ph·ªëi l·ªõp):

```
Class 0 (Normal):  284,315 samples (99.83%)
Class 1 (Fraud):       492 samples (0.17%)
Imbalance ratio: 0.0017 (fraud/normal)
```

**Ph√¢n t√≠ch class imbalance**:
- ƒê√¢y l√† m·ªôt trong nh·ªØng dataset c√≥ **class imbalance nghi√™m tr·ªçng nh·∫•t**
- T·ª∑ l·ªá 1:578 (1 giao d·ªãch gian l·∫≠n tr√™n 578 giao d·ªãch b√¨nh th∆∞·ªùng)
- ƒêi·ªÅu n√†y khi·∫øn vi·ªác ƒë√°nh gi√° m√¥ h√¨nh tr·ªü n√™n kh√≥ khƒÉn:
  - Accuracy kh√¥ng ph·∫£i l√† metric t·ªët (m√¥ h√¨nh ch·ªâ c·∫ßn d·ª± ƒëo√°n t·∫•t c·∫£ l√† "Normal" c≈©ng ƒë·∫°t 99.83% accuracy)
  - C·∫ßn t·∫≠p trung v√†o **Precision**, **Recall**, **F1-Score** v√† **AUC**

#### ƒê·∫∑c ƒëi·ªÉm ph√¢n ph·ªëi:

**Time Feature**:
- Mean: 94,813.86 gi√¢y (~26.34 gi·ªù)
- Median: 84,692 gi√¢y (~23.53 gi·ªù)
- Ph√¢n ph·ªëi: H∆°i l·ªách tr√°i (Skewness ‚âà -0.036)
- **Insight**: C√≥ pattern theo chu k·ª≥ ng√†y/ƒë√™m, t·ª∑ l·ªá gian l·∫≠n cao h∆°n v√†o ban ƒë√™m (2-4h s√°ng)

**Amount Feature**:
- Mean: $88.35
- Median: $22.00
- Max: $25,691.16
- **Ph√¢n ph·ªëi l·ªách ph·∫£i nghi√™m tr·ªçng**:
  - Skewness: 16.98 (r·∫•t cao)
  - Kurtosis: 845.07 (ph√¢n ph·ªëi c·ª±c k·ª≥ nh·ªçn)
- **So s√°nh Normal vs Fraud**:
  - Normal transactions: Mean = $88.29, Median = $22.00
  - Fraud transactions: Mean = $122.21, Median = $9.25
  - **K·∫øt lu·∫≠n**: Giao d·ªãch gian l·∫≠n c√≥ gi√° tr·ªã trung b√¨nh cao h∆°n nh∆∞ng median th·∫•p h∆°n

**PCA Features (V1-V28)**:
- T·∫•t c·∫£ ƒë·ªÅu c√≥ mean ‚âà 0 (do ƒë√£ ƒë∆∞·ª£c PCA transform)
- Standard deviation gi·∫£m d·∫ßn t·ª´ V1 ƒë·∫øn V28 (t·ª´ 1.96 xu·ªëng 0.33)
- **T√≠nh tr·ª±c giao**: C√°c features n√†y h·∫ßu nh∆∞ kh√¥ng t∆∞∆°ng quan v·ªõi nhau (ƒë·∫∑c t√≠nh c·ªßa PCA)
- **Top features quan tr·ªçng nh·∫•t** (d·ª±a tr√™n s·ª± kh√°c bi·ªát gi·ªØa Normal v√† Fraud):
  1. V3: Diff = 7.05
  2. V14: Diff = 6.98
  3. V17: Diff = 6.68
  4. V12: Diff = 6.27
  5. V10: Diff = 5.69

---

## 3. Method

### 3.1. Quy tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu

#### 3.1.1. Data Loading

**S·ª≠ d·ª•ng NumPy ƒë·ªÉ ƒë·ªçc CSV** (kh√¥ng d√πng Pandas):

```python
# ƒê·ªçc file CSV b·∫±ng np.genfromtxt
data_str = np.genfromtxt(file_path, dtype=str, delimiter=',')

# X·ª≠ l√Ω header
data_str = np.char.strip(data_str, '"')
header = data_str[0]
data_str = data_str[1:]

# Convert sang float64
data = data_str.astype(np.float64)
```

**K·∫øt qu·∫£**: Ma tr·∫≠n d·ªØ li·ªáu shape (284807, 31) - 30 features + 1 target

#### 3.1.2. Data Exploration

**a) Ki·ªÉm tra d·ªØ li·ªáu thi·∫øu**:
```python
missing_mask = np.isnan(data) | np.isinf(data)
missing_count = np.sum(missing_mask, axis=0)
# K·∫øt qu·∫£: 0 missing values
```

**b) T√≠nh to√°n th·ªëng k√™ m√¥ t·∫£**:
- Mean, Median, Std, Variance
- Min, Max, Quartiles (Q1, Q2, Q3)
- Skewness v√† Kurtosis (implement t·ª´ ƒë·∫ßu b·∫±ng NumPy)

**c) Ph√¢n t√≠ch class distribution**:
- ƒê·∫øm s·ªë l∆∞·ª£ng samples m·ªói class
- T√≠nh t·ª∑ l·ªá ph·∫ßn trƒÉm
- Visualize b·∫±ng bar chart v√† pie chart

**d) Ph√¢n t√≠ch features**:
- Ph√¢n t√≠ch Time feature: Chuy·ªÉn ƒë·ªïi sang gi·ªù, ph√¢n t√≠ch theo chu k·ª≥ ng√†y/ƒë√™m
- Ph√¢n t√≠ch Amount feature: So s√°nh gi·ªØa Normal v√† Fraud
- Ph√¢n t√≠ch PCA features: Visualize ph√¢n ph·ªëi c·ªßa V1-V9

**e) Correlation analysis**:
```python
# T√≠nh correlation matrix b·∫±ng NumPy
mean = np.mean(data, axis=0, keepdims=True)
std = np.std(data, axis=0, keepdims=True)
data_std = (data - mean) / std
corr_matrix = np.corrcoef(data_std.T)
```

**f) Feature importance**:
- So s√°nh gi√° tr·ªã trung b√¨nh gi·ªØa Normal v√† Fraud
- X√°c ƒë·ªãnh top features c√≥ s·ª± kh√°c bi·ªát l·ªõn nh·∫•t

**g) Statistical hypothesis testing**:
- T-test ƒë·ªÉ ki·ªÉm tra s·ª± kh√°c bi·ªát v·ªÅ Amount gi·ªØa Normal v√† Fraud
- K·∫øt qu·∫£: p-value = 0.0034 < 0.05 ‚Üí B√°c b·ªè H0, c√≥ s·ª± kh√°c bi·ªát c√≥ √Ω nghƒ©a th·ªëng k√™

#### 3.1.3. Data Preprocessing

**a) Missing Values Handling**:

M·∫∑c d√π dataset kh√¥ng c√≥ missing values, nh∆∞ng ƒë√£ implement c√°c ph∆∞∆°ng ph√°p x·ª≠ l√Ω:

1. **Mean Imputation**:
```python
mean_val = np.nanmean(col_data)
data[missing_mask, i] = mean_val
```

2. **Median Imputation**:
```python
median_val = np.nanmedian(col_data)
data[missing_mask, i] = median_val
```

3. **Specific Value Imputation**:
```python
data[np.isnan(data)] = -999
```

4. **Linear Regression Imputation** (cho Amount d·ª±a tr√™n Time):
```python
# Normal Equation: Œ≤ = (X^T X)^(-1) X^T y
X_reg = np.column_stack([np.ones(len(time_valid)), time_valid])
beta = np.linalg.solve(X_reg.T @ X_reg, X_reg.T @ y_reg)
predicted_amount = X_pred @ beta
```

**b) Outlier Detection**:

**Ph∆∞∆°ng ph√°p 1: IQR Method**
```python
q1 = np.percentile(X, 25, axis=0)
q3 = np.percentile(X, 75, axis=0)
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr
outlier_mask = (X < lower_bound) | (X > upper_bound)
```

**K·∫øt qu·∫£**: 370,372 outliers (4.33% t·ªïng s·ªë data points)

**Ph∆∞∆°ng ph√°p 2: Z-score Method**
```python
mean_vals = np.mean(X, axis=0, keepdims=True)
std_vals = np.std(X, axis=0, ddof=1, keepdims=True)
z_scores = (X - mean_vals) / std_vals
outlier_mask = np.abs(z_scores) > 3.0
```

**K·∫øt qu·∫£**: 83,598 outliers (0.98% t·ªïng s·ªë data points)

**Quy·∫øt ƒë·ªãnh**: **KH√îNG lo·∫°i b·ªè outliers** v√¨:
- Trong b√†i to√°n fraud detection, outliers c√≥ th·ªÉ ch√≠nh l√† c√°c giao d·ªãch gian l·∫≠n
- Lo·∫°i b·ªè outliers c√≥ th·ªÉ l√†m m·∫•t ƒëi nh·ªØng m·∫´u quan tr·ªçng nh·∫•t
- Thay v√†o ƒë√≥, s·ª≠ d·ª•ng c√°c ph∆∞∆°ng ph√°p chu·∫©n h√≥a m·∫°nh (robust scaling)

**c) Normalization & Standardization**:

**B∆∞·ªõc 1: Log Transformation cho Amount**
```python
# X·ª≠ l√Ω ph√¢n ph·ªëi l·ªách ph·∫£i
X_processed[:, amount_idx] = np.log1p(X[:, amount_idx])
```

**L√Ω do**: 
- Amount c√≥ skewness = 16.98 (r·∫•t cao)
- Sau log transform: skewness gi·∫£m xu·ªëng 0.16
- Gi√∫p ph√¢n ph·ªëi g·∫ßn v·ªõi chu·∫©n h∆°n

**B∆∞·ªõc 2: Z-score Standardization**
```python
mean_vals = np.mean(X_processed, axis=0, keepdims=True)
std_vals = np.std(X_processed, axis=0, ddof=1, keepdims=True)
std_vals = np.where(std_vals == 0, 1, std_vals)  # Tr√°nh chia cho 0
X_processed = (X_processed - mean_vals) / std_vals
```

**K·∫øt qu·∫£**: 
- Mean ‚âà 0, Std ‚âà 1 cho t·∫•t c·∫£ features
- Ph√π h·ª£p v·ªõi c√°c thu·∫≠t to√°n d·ª±a tr√™n gradient (Logistic Regression)

**C√°c ph∆∞∆°ng ph√°p kh√°c ƒë√£ th·ª≠ nghi·ªám**:
- **Min-Max Normalization**: ƒê∆∞a v·ªÅ [0, 1], nh∆∞ng b·ªã ·∫£nh h∆∞·ªüng m·∫°nh b·ªüi outliers
- **Decimal Scaling**: √çt ph·ªï bi·∫øn, k√©m hi·ªáu qu·∫£ h∆°n Z-score

**d) Train-Test Split**:

```python
test_size = 0.2
random_state = 42

np.random.seed(random_state)
indices = np.arange(n_samples)
np.random.shuffle(indices)

n_test = int(n_samples * test_size)
test_indices = indices[:n_test]
train_indices = indices[n_test:]

X_train = X_processed[train_indices]
X_test = X_processed[test_indices]
y_train = y[train_indices]
y_test = y[test_indices]
```

**K·∫øt qu·∫£**:
- Train set: 227,846 samples (80%)
- Test set: 56,961 samples (20%)
- **B·∫£o to√†n class distribution**: 
  - Train: 99.83% Normal, 0.17% Fraud
  - Test: 99.83% Normal, 0.17% Fraud

### 3.2. Thu·∫≠t to√°n s·ª≠ d·ª•ng

#### 3.2.1. Logistic Regression

**C√¥ng th·ª©c to√°n h·ªçc**:

**1. Sigmoid Function**:
$$P(y=1|x) = \sigma(z) = \frac{1}{1 + e^{-z}}$$

v·ªõi $z = w^T x + b = \sum_{i=1}^{n} w_i x_i + b$

**2. Loss Function (Binary Cross-Entropy)**:
$$L = -\frac{1}{m}\sum_{i=1}^{m}[y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i)]$$

Trong ƒë√≥:
- $m$: s·ªë l∆∞·ª£ng samples
- $y_i$: nh√£n th·ª±c t·∫ø (0 ho·∫∑c 1)
- $\hat{y}_i = \sigma(w^T x_i + b)$: x√°c su·∫•t d·ª± ƒëo√°n

**3. Gradient Computation**:

ƒê·∫°o h√†m c·ªßa loss function theo weights:
$$\frac{\partial L}{\partial w} = \frac{1}{m}X^T(\hat{y} - y)$$

ƒê·∫°o h√†m c·ªßa loss function theo bias:
$$\frac{\partial L}{\partial b} = \frac{1}{m}\sum_{i=1}^{m}(\hat{y}_i - y_i)$$

**4. Update Rules (Gradient Descent)**:
$$w := w - \alpha \frac{\partial L}{\partial w}$$
$$b := b - \alpha \frac{\partial L}{\partial b}$$

Trong ƒë√≥ $\alpha$ l√† learning rate.

**Implementation b·∫±ng NumPy**:

```python
class LogisticRegression:
    def __init__(self, learning_rate=0.01, max_iter=1000, tol=1e-6):
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.tol = tol
        self.weights = None
        self.bias = None
        
    def _sigmoid(self, z):
        # Clip ƒë·ªÉ tr√°nh overflow
        z = np.clip(z, -500, 500)
        return 1 / (1 + np.exp(-z))
    
    def fit(self, X, y):
        m, n = X.shape
        
        # Kh·ªüi t·∫°o weights ng·∫´u nhi√™n nh·ªè
        self.weights = np.random.randn(n) * 0.01
        self.bias = 0.0
        
        for i in range(self.max_iter):
            # Forward pass
            z = X @ self.weights + self.bias
            y_pred = self._sigmoid(z)
            
            # T√≠nh loss
            loss = -np.mean(y * np.log(y_pred + 1e-15) + 
                           (1 - y) * np.log(1 - y_pred + 1e-15))
            
            # Backward pass (Gradient computation)
            dw = (1/m) * X.T @ (y_pred - y)
            db = (1/m) * np.sum(y_pred - y)
            
            # Update weights
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # Ki·ªÉm tra convergence
            if i > 0 and abs(prev_loss - loss) < self.tol:
                break
            prev_loss = loss
    
    def predict_proba(self, X):
        z = X @ self.weights + self.bias
        return self._sigmoid(z)
    
    def predict(self, X, threshold=0.5):
        probabilities = self.predict_proba(X)
        return (probabilities >= threshold).astype(int)
```

**ƒê·∫∑c ƒëi·ªÉm implementation**:
- **Vectorized operations**: T·∫•t c·∫£ t√≠nh to√°n ƒë·ªÅu ƒë∆∞·ª£c vectorize, kh√¥ng d√πng for loops
- **Broadcasting**: S·ª≠ d·ª•ng broadcasting ƒë·ªÉ t√≠nh to√°n hi·ªáu qu·∫£
- **Numerical stability**: Clip z values ƒë·ªÉ tr√°nh overflow trong sigmoid
- **Epsilon trong log**: Th√™m 1e-15 ƒë·ªÉ tr√°nh log(0)

**Hyperparameters**:
- Learning rate: 0.01
- Max iterations: 1000
- Tolerance: 1e-6 (ƒë·ªÉ ki·ªÉm tra convergence)
- Random state: 42 (ƒë·∫£m b·∫£o reproducibility)

### 3.3. Evaluation Metrics

Trong b√†i to√°n imbalanced data, **Accuracy kh√¥ng ph·∫£i l√† metric t·ªët**. C√°c metrics quan tr·ªçng:

**1. Confusion Matrix**:

|                | Predicted Normal | Predicted Fraud |
|----------------|------------------|-----------------|
| **Actual Normal** | TN (True Negative) | FP (False Positive) |
| **Actual Fraud**  | FN (False Negative) | TP (True Positive) |

**2. Precision (ƒê·ªô ch√≠nh x√°c d∆∞∆°ng t√≠nh)**:
$$\text{Precision} = \frac{TP}{TP + FP}$$

√ù nghƒ©a: Trong s·ªë c√°c giao d·ªãch m√¥ h√¨nh d·ª± ƒëo√°n l√† gian l·∫≠n, bao nhi√™u ph·∫ßn trƒÉm l√† ƒë√∫ng?

**3. Recall (ƒê·ªô nh·∫°y)**:
$$\text{Recall} = \frac{TP}{TP + FN}$$

√ù nghƒ©a: M√¥ h√¨nh ph√°t hi·ªán ƒë∆∞·ª£c bao nhi√™u ph·∫ßn trƒÉm t·ªïng s·ªë v·ª• gian l·∫≠n th·ª±c t·∫ø?

**4. F1-Score (Trung b√¨nh ƒëi·ªÅu h√≤a)**:
$$\text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

**5. AUC (Area Under ROC Curve)**:
- ROC Curve: V·∫Ω True Positive Rate (Recall) vs False Positive Rate
- AUC: Di·ªán t√≠ch d∆∞·ªõi ƒë∆∞·ªùng cong ROC
- Metric t·ªët nh·∫•t cho imbalanced data v√¨ kh√¥ng ph·ª• thu·ªôc v√†o threshold

**Implementation b·∫±ng NumPy**:

```python
def confusion_matrix(y_true, y_pred):
    tn = np.sum((y_true == 0) & (y_pred == 0))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))
    tp = np.sum((y_true == 1) & (y_pred == 1))
    return np.array([[tn, fp], [fn, tp]])

def precision_score(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    tp, fp = cm[1, 1], cm[0, 1]
    return tp / (tp + fp) if (tp + fp) > 0 else 0.0

def recall_score(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    tp, fn = cm[1, 1], cm[1, 0]
    return tp / (tp + fn) if (tp + fn) > 0 else 0.0

def roc_curve(y_true, y_scores):
    # S·∫Øp x·∫øp theo score gi·∫£m d·∫ßn
    sorted_indices = np.argsort(y_scores)[::-1]
    y_true_sorted = y_true[sorted_indices]
    y_scores_sorted = y_scores[sorted_indices]
    
    # T√≠nh FPR v√† TPR cho t·ª´ng threshold
    thresholds = np.unique(y_scores_sorted)
    fpr, tpr = [], []
    
    for threshold in thresholds:
        y_pred = (y_scores_sorted >= threshold).astype(int)
        cm = confusion_matrix(y_true_sorted, y_pred)
        tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]
        fpr.append(fp / (fp + tn) if (fp + tn) > 0 else 0.0)
        tpr.append(tp / (tp + fn) if (tp + fn) > 0 else 0.0)
    
    return np.array(fpr), np.array(tpr), thresholds

def auc_score(fpr, tpr):
    # T√≠nh di·ªán t√≠ch b·∫±ng ph∆∞∆°ng ph√°p trapezoidal
    sorted_indices = np.argsort(fpr)
    fpr_sorted = fpr[sorted_indices]
    tpr_sorted = tpr[sorted_indices]
    return np.trapz(tpr_sorted, fpr_sorted)
```

---

## 4. Installation & Setup

### 4.1. Requirements

- **Python**: 3.7 tr·ªü l√™n
- **NumPy**: >= 1.21.0
- **Matplotlib**: >= 3.5.0 (cho visualization)
- **Seaborn**: >= 0.11.0 (cho visualization ƒë·∫πp h∆°n)
- **Jupyter**: >= 1.0.0 (ƒë·ªÉ ch·∫°y notebooks)

### 4.2. Installation

**B∆∞·ªõc 1: Clone repository** (n·∫øu c√≥)
```bash
git clone <repository-url>
cd <project-directory>
```

**B∆∞·ªõc 2: T·∫°o virtual environment** (khuy·∫øn ngh·ªã)
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

**B∆∞·ªõc 3: Install dependencies**
```bash
pip install -r requirements.txt
```

Ho·∫∑c install t·ª´ng package:
```bash
pip install numpy>=1.21.0 matplotlib>=3.5.0 seaborn>=0.11.0 jupyter
```

### 4.3. Dataset Setup

1. **Download dataset**:
   - Truy c·∫≠p [Kaggle - Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)
   - Download file `creditcard.csv`

2. **ƒê·∫∑t file v√†o ƒë√∫ng th∆∞ m·ª•c**:
   ```
   data/
   ‚îî‚îÄ‚îÄ raw/
       ‚îî‚îÄ‚îÄ creditcard.csv
   ```

3. **Ki·ªÉm tra c·∫•u tr√∫c th∆∞ m·ª•c**:
   ```
   project/
   ‚îú‚îÄ‚îÄ data/
   ‚îÇ   ‚îú‚îÄ‚îÄ raw/
   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ creditcard.csv
   ‚îÇ   ‚îî‚îÄ‚îÄ processed/  (s·∫Ω ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông)
   ‚îú‚îÄ‚îÄ notebooks/
   ‚îÇ   ‚îú‚îÄ‚îÄ 01_data_exploration.ipynb
   ‚îÇ   ‚îú‚îÄ‚îÄ 02_preprocessing.ipynb
   ‚îÇ   ‚îî‚îÄ‚îÄ 03_modeling.ipynb
   ‚îú‚îÄ‚îÄ requirements.txt
   ‚îî‚îÄ‚îÄ README.md
   ```

---

## 5. Usage

### 5.1. H∆∞·ªõng d·∫´n c√°ch ch·∫°y t·ª´ng ph·∫ßn

#### 5.1.1. Data Exploration

**Ch·∫°y notebook ƒë·∫ßu ti√™n**:
```bash
jupyter notebook notebooks/01_data_exploration.ipynb
```

**Notebook n√†y s·∫Ω th·ª±c hi·ªán**:
1. Load dataset t·ª´ `data/raw/creditcard.csv`
2. Ki·ªÉm tra missing values v√† th·ªëng k√™ c∆° b·∫£n
3. Ph√¢n t√≠ch class distribution
4. Ph√¢n t√≠ch c√°c features quan tr·ªçng:
   - Time feature: Ph√¢n t√≠ch theo chu k·ª≥ ng√†y/ƒë√™m
   - Amount feature: So s√°nh gi·ªØa Normal v√† Fraud
   - PCA features (V1-V28): Ph√¢n t√≠ch ph√¢n ph·ªëi
5. Correlation analysis gi·ªØa c√°c features
6. So s√°nh features gi·ªØa Normal v√† Fraud transactions
7. Feature engineering: T·∫°o rolling statistics
8. Statistical hypothesis testing (T-test)
9. X·ª≠ l√Ω missing values (demo c√°c ph∆∞∆°ng ph√°p)
10. L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o `data/processed/`

**K·∫øt qu·∫£ ƒë·∫ßu ra**:
- File `header.npy`: T√™n c√°c features
- File `X_regression_filled.npy`: D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω missing values (n·∫øu c√≥)

#### 5.1.2. Data Preprocessing

**Ch·∫°y notebook th·ª© hai**:
```bash
jupyter notebook notebooks/02_preprocessing.ipynb
```

**Notebook n√†y s·∫Ω th·ª±c hi·ªán**:
1. Load d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω t·ª´ notebook 01
2. **Outlier Detection**:
   - IQR Method
   - Z-score Method
   - So s√°nh v√† ph√¢n t√≠ch k·∫øt qu·∫£
3. **Normalization & Standardization**:
   - Min-Max Normalization
   - Z-score Standardization
   - Log Transformation
   - Decimal Scaling
   - So s√°nh c√°c ph∆∞∆°ng ph√°p
4. **√Åp d·ª•ng preprocessing cu·ªëi c√πng**:
   - Log transform cho Amount
   - Z-score standardization cho t·∫•t c·∫£ features
5. **Train-Test Split**:
   - Chia 80% train, 20% test
   - B·∫£o to√†n class distribution
6. L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω:
   - `X_processed.npy`: D·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a
   - `y.npy`: Labels
   - `X_train.npy`, `X_test.npy`: Train/test features
   - `y_train.npy`, `y_test.npy`: Train/test labels

**K·∫øt qu·∫£ ƒë·∫ßu ra**:
- C√°c file `.npy` trong `data/processed/` ƒë·ªÉ s·ª≠ d·ª•ng cho modeling

#### 5.1.3. Modeling

**Ch·∫°y notebook th·ª© ba**:
```bash
jupyter notebook notebooks/03_modeling.ipynb
```

**Notebook n√†y s·∫Ω th·ª±c hi·ªán**:
1. Load d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω t·ª´ notebook 02
2. **Implement Evaluation Metrics**:
   - Accuracy, Precision, Recall, F1-Score
   - Confusion Matrix
   - ROC Curve v√† AUC
3. **Implement Logistic Regression**:
   - Class LogisticRegression v·ªõi Gradient Descent
   - Training v·ªõi c√°c hyperparameters
   - Visualize training loss history
4. **Evaluation**:
   - D·ª± ƒëo√°n tr√™n test set
   - T√≠nh c√°c metrics
   - V·∫Ω Confusion Matrix
   - V·∫Ω ROC Curve v√† t√≠nh AUC
5. **Ph√¢n t√≠ch k·∫øt qu·∫£**:
   - Ph√¢n t√≠ch quantitative metrics
   - Ph√¢n t√≠ch Confusion Matrix
   - Ph√¢n t√≠ch ROC Curve v√† AUC

**K·∫øt qu·∫£ ƒë·∫ßu ra**:
- C√°c bi·ªÉu ƒë·ªì visualization
- Metrics tr√™n test set
- Ph√¢n t√≠ch v√† ƒë√°nh gi√° m√¥ h√¨nh

### 5.2. Ch·∫°y tu·∫ßn t·ª± to√†n b·ªô pipeline

**C√°ch 1: Ch·∫°y t·ª´ng notebook theo th·ª© t·ª±**
1. Ch·∫°y `01_data_exploration.ipynb` ‚Üí L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
2. Ch·∫°y `02_preprocessing.ipynb` ‚Üí L∆∞u d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a v√† split
3. Ch·∫°y `03_modeling.ipynb` ‚Üí Hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh

**C√°ch 2: S·ª≠ d·ª•ng Jupyter Notebook v·ªõi kernel**
- M·ªü Jupyter Notebook
- Ch·∫°y t·∫•t c·∫£ cells trong t·ª´ng notebook theo th·ª© t·ª±

### 5.3. L∆∞u √Ω quan tr·ªçng

‚ö†Ô∏è **Th·ª© t·ª± ch·∫°y**: Ph·∫£i ch·∫°y theo th·ª© t·ª± 01 ‚Üí 02 ‚Üí 03 v√¨:
- Notebook 02 ph·ª• thu·ªôc v√†o output c·ªßa notebook 01
- Notebook 03 ph·ª• thu·ªôc v√†o output c·ªßa notebook 02

‚ö†Ô∏è **Dataset**: ƒê·∫£m b·∫£o file `creditcard.csv` ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t trong `data/raw/` tr∆∞·ªõc khi ch·∫°y

‚ö†Ô∏è **Memory**: Dataset kh√° l·ªõn (~150MB), ƒë·∫£m b·∫£o c√≥ ƒë·ªß RAM

---

## 7. Results

### 7.1. K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c (Metrics)

#### 7.1.1. Logistic Regression

**Hyperparameters**:
- Learning rate: 0.01
- Max iterations: 1000
- Random state: 42

**Training Results**:
- S·ªë iterations th·ª±c t·∫ø: 1000 (ch∆∞a converge, nh∆∞ng loss ƒë√£ ·ªïn ƒë·ªãnh)
- Final training loss: **0.1095**
- Training loss gi·∫£m ƒë·ªÅu v√† m∆∞·ª£t m√†, kh√¥ng c√≥ d·∫•u hi·ªáu overfitting

**Test Results**:

| Metric | Value | Gi·∫£i th√≠ch |
|--------|-------|------------|
| **Accuracy** | 0.9989 | R·∫•t cao nh∆∞ng kh√¥ng c√≥ √Ω nghƒ©a trong b√†i to√°n imbalanced |
| **Precision** | 0.8333 | T·ªët - 83.33% c·∫£nh b√°o l√† ƒë√∫ng |
| **Recall** | 0.4592 | Th·∫•p - Ch·ªâ ph√°t hi·ªán ƒë∆∞·ª£c 45.92% t·ªïng s·ªë gian l·∫≠n |
| **F1-Score** | 0.5921 | Trung b√¨nh - B·ªã k√©o xu·ªëng do Recall th·∫•p |
| **AUC** | **0.9748** | R·∫•t cao - M√¥ h√¨nh c√≥ kh·∫£ nƒÉng ph√¢n lo·∫°i t·ªët |

**Confusion Matrix**:

|                | Predicted Normal | Predicted Fraud |
|----------------|------------------|-----------------|
| **Actual Normal** | 56,854 (TN) | 9 (FP) |
| **Actual Fraud**  | 53 (FN) | 45 (TP) |

**Ph√¢n t√≠ch**:
- ‚úÖ **True Negatives (56,854)**: ƒêa s·ªë giao d·ªãch b√¨nh th∆∞·ªùng ƒë∆∞·ª£c ph√¢n lo·∫°i ƒë√∫ng
- ‚úÖ **True Positives (45)**: Ph√°t hi·ªán ƒë∆∞·ª£c 45/98 v·ª• gian l·∫≠n (45.92%)
- ‚ö†Ô∏è **False Positives (9)**: Ch·ªâ c√≥ 9 c·∫£nh b√°o gi·∫£ - Precision cao
- ‚ùå **False Negatives (53)**: **53 v·ª• gian l·∫≠n b·ªã b·ªè s√≥t** - ƒê√¢y l√† v·∫•n ƒë·ªÅ l·ªõn nh·∫•t

**Nh·∫≠n ƒë·ªãnh**:
- M√¥ h√¨nh ƒëang **thi√™n v·ªÅ Precision** (an to√†n qu√° m·ª©c)
- **Recall th·∫•p** l√† v·∫•n ƒë·ªÅ nghi√™m tr·ªçng trong b√†i to√°n fraud detection
- Tuy nhi√™n, **AUC cao (0.9748)** ch·ª©ng t·ªè m√¥ h√¨nh c√≥ kh·∫£ nƒÉng ph√¢n lo·∫°i t·ªët
- V·∫•n ƒë·ªÅ n·∫±m ·ªü **threshold qu√° cao (0.5)** - c√≥ th·ªÉ h·∫° xu·ªëng ƒë·ªÉ tƒÉng Recall

### 7.2. H√¨nh ·∫£nh tr·ª±c quan h√≥a k·∫øt qu·∫£

#### 7.2.1. Data Exploration Visualizations

**1. Class Distribution**:
- Bar chart: So s√°nh s·ªë l∆∞·ª£ng Normal vs Fraud
- Pie chart: T·ª∑ l·ªá ph·∫ßn trƒÉm c·ªßa m·ªói class
- **Insight**: Class imbalance nghi√™m tr·ªçng (99.83% vs 0.17%)

**2. Time Feature Analysis**:
- Histogram: Ph√¢n ph·ªëi giao d·ªãch theo gi·ªù
- Boxplot: So s√°nh Time gi·ªØa Normal v√† Fraud
- Line chart: Fraud rate theo gi·ªù trong ng√†y
- **Insight**: Fraud rate cao h∆°n v√†o ban ƒë√™m (2-4h s√°ng)

**3. Amount Feature Analysis**:
- Histogram: Ph√¢n ph·ªëi Amount (l·ªách ph·∫£i nghi√™m tr·ªçng)
- Boxplot: So s√°nh Amount gi·ªØa Normal v√† Fraud
- **Insight**: Fraud transactions c√≥ mean cao h∆°n nh∆∞ng median th·∫•p h∆°n

**4. PCA Features Distribution**:
- Histograms cho V1-V9: Ph√¢n ph·ªëi c·ªßa c√°c PCA features
- **Insight**: C√°c features ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a, mean ‚âà 0

**5. Correlation Heatmap**:
- Heatmap t∆∞∆°ng quan gi·ªØa c√°c features quan tr·ªçng
- **Insight**: PCA features kh√¥ng t∆∞∆°ng quan v·ªõi nhau (t√≠nh tr·ª±c giao)

**6. Feature Engineering - Rolling Statistics**:
- Line chart: Amount v·ªõi Rolling Mean v√† Rolling Std
- Scatter plot: Anomaly detection b·∫±ng Z-Score
- **Insight**: C√≥ th·ªÉ ph√°t hi·ªán anomalies c·ª•c b·ªô

#### 7.2.2. Preprocessing Visualizations

**1. Outlier Detection**:
- So s√°nh s·ªë l∆∞·ª£ng outliers gi·ªØa IQR v√† Z-score methods
- **Insight**: IQR ph√°t hi·ªán nhi·ªÅu outliers h∆°n Z-score

**2. Normalization Comparison**:
- Histograms so s√°nh: Original vs Min-Max vs Z-score vs Log-transformed
- **Insight**: Log transformation gi·∫£m skewness t·ª´ 16.98 xu·ªëng 0.16

#### 7.2.3. Modeling Visualizations

**1. Training Loss History**:
- Line chart: Loss gi·∫£m ƒë·ªÅu qua c√°c iterations
- **Insight**: M√¥ h√¨nh h·ªôi t·ª• t·ªët, kh√¥ng c√≥ d·∫•u hi·ªáu overfitting

**2. Confusion Matrix**:
- Heatmap: Tr·ª±c quan h√≥a s·ªë l∆∞·ª£ng TP, TN, FP, FN
- **Insight**: False Negatives cao (53) l√† v·∫•n ƒë·ªÅ ch√≠nh

**3. ROC Curve**:
- Line chart: ROC curve v·ªõi AUC = 0.9748
- So s√°nh v·ªõi Random Classifier (ƒë∆∞·ªùng ch√©o)
- **Insight**: M√¥ h√¨nh c√≥ kh·∫£ nƒÉng ph√¢n lo·∫°i r·∫•t t·ªët

### 7.3. So s√°nh v√† ph√¢n t√≠ch

#### 7.3.1. ƒêi·ªÉm m·∫°nh c·ªßa m√¥ h√¨nh

1. **AUC Score cao (0.9748)**:
   - Ch·ª©ng t·ªè m√¥ h√¨nh c√≥ kh·∫£ nƒÉng ph√¢n bi·ªát t·ªët gi·ªØa Normal v√† Fraud
   - Top 5% trong c√°c m√¥ h√¨nh fraud detection

2. **Precision cao (0.8333)**:
   - Gi·∫£m thi·ªÉu False Positives
   - Kh√¥ng l√†m phi·ªÅn kh√°ch h√†ng b·∫±ng c·∫£nh b√°o gi·∫£

3. **Training ·ªïn ƒë·ªãnh**:
   - Loss gi·∫£m ƒë·ªÅu, kh√¥ng c√≥ d·∫•u hi·ªáu overfitting
   - Gradient Descent ho·∫°t ƒë·ªông t·ªët

#### 7.3.2. ƒêi·ªÉm y·∫øu v√† v·∫•n ƒë·ªÅ

1. **Recall th·∫•p (0.4592)**:
   - Ch·ªâ ph√°t hi·ªán ƒë∆∞·ª£c 45.92% t·ªïng s·ªë gian l·∫≠n
   - **53 v·ª• gian l·∫≠n b·ªã b·ªè s√≥t** - g√¢y thi·ªát h·∫°i t√†i ch√≠nh

2. **Threshold qu√° cao**:
   - Threshold m·∫∑c ƒë·ªãnh 0.5 c√≥ th·ªÉ kh√¥ng ph√π h·ª£p
   - C·∫ßn tune threshold ƒë·ªÉ c√¢n b·∫±ng Precision v√† Recall

3. **Class imbalance**:
   - M√¥ h√¨nh thi√™n v·ªÅ class ƒëa s·ªë (Normal)
   - C·∫ßn x·ª≠ l√Ω class imbalance t·ªët h∆°n

#### 7.3.3. So s√°nh v·ªõi Baseline

**Baseline (D·ª± ƒëo√°n t·∫•t c·∫£ l√† Normal)**:
- Accuracy: 0.9983
- Precision: 0.0 (kh√¥ng c√≥ TP)
- Recall: 0.0 (kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c fraud n√†o)
- F1-Score: 0.0

**M√¥ h√¨nh Logistic Regression**:
- Accuracy: 0.9989 (+0.0006)
- Precision: 0.8333 (t·ªët)
- Recall: 0.4592 (t·ªët h∆°n baseline r·∫•t nhi·ªÅu)
- F1-Score: 0.5921 (t·ªët)

**K·∫øt lu·∫≠n**: M√¥ h√¨nh t·ªët h∆°n baseline ƒë√°ng k·ªÉ, ƒë·∫∑c bi·ªát l√† c√≥ th·ªÉ ph√°t hi·ªán ƒë∆∞·ª£c fraud.

#### 7.3.4. Insights quan tr·ªçng

1. **PCA Features quan tr·ªçng**:
   - V3, V14, V17, V12, V10 l√† nh·ªØng features quan tr·ªçng nh·∫•t
   - C√≥ s·ª± kh√°c bi·ªát l·ªõn gi·ªØa Normal v√† Fraud

2. **Time pattern**:
   - Fraud rate cao h∆°n v√†o ban ƒë√™m (2-4h s√°ng)
   - C√≥ th·ªÉ s·ª≠ d·ª•ng l√†m feature engineering

3. **Amount distribution**:
   - Fraud c√≥ mean cao h∆°n nh∆∞ng median th·∫•p h∆°n
   - C·∫ßn log transformation ƒë·ªÉ x·ª≠ l√Ω skewness

4. **Threshold optimization**:
   - AUC cao ch·ª©ng t·ªè c√≥ th·ªÉ tune threshold ƒë·ªÉ c·∫£i thi·ªán Recall
   - Trade-off gi·ªØa Precision v√† Recall

---

## 8. Project Structure

```
23127516/
‚îú‚îÄ‚îÄ README.md                          # File README n√†y
‚îú‚îÄ‚îÄ requirements.txt                   # Danh s√°ch c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
‚îÇ
‚îú‚îÄ‚îÄ data/                              # Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu
‚îÇ   ‚îú‚îÄ‚îÄ raw/                           # D·ªØ li·ªáu g·ªëc
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ creditcard.csv             # Dataset g·ªëc t·ª´ Kaggle
‚îÇ   ‚îî‚îÄ‚îÄ processed/                     # D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
‚îÇ       ‚îú‚îÄ‚îÄ header.npy                 # T√™n c√°c features
‚îÇ       ‚îú‚îÄ‚îÄ X_mean_filled.npy          # D·ªØ li·ªáu ƒëi·ªÅn b·∫±ng mean
‚îÇ       ‚îú‚îÄ‚îÄ X_median_filled.npy        # D·ªØ li·ªáu ƒëi·ªÅn b·∫±ng median
‚îÇ       ‚îú‚îÄ‚îÄ X_regression_filled.npy    # D·ªØ li·ªáu ƒëi·ªÅn b·∫±ng regression
‚îÇ       ‚îú‚îÄ‚îÄ X_specific_filled.npy       # D·ªØ li·ªáu ƒëi·ªÅn b·∫±ng gi√° tr·ªã c·ª• th·ªÉ
‚îÇ       ‚îú‚îÄ‚îÄ X_processed.npy            # D·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a (log + z-score)
‚îÇ       ‚îú‚îÄ‚îÄ y.npy                      # Labels
‚îÇ       ‚îú‚îÄ‚îÄ X_train.npy                # Features t·∫≠p train
‚îÇ       ‚îú‚îÄ‚îÄ X_test.npy                 # Features t·∫≠p test
‚îÇ       ‚îú‚îÄ‚îÄ y_train.npy                # Labels t·∫≠p train
‚îÇ       ‚îî‚îÄ‚îÄ y_test.npy                 # Labels t·∫≠p test
‚îÇ
‚îî‚îÄ‚îÄ notebooks/                         # Th∆∞ m·ª•c ch·ª©a c√°c Jupyter notebooks
    ‚îú‚îÄ‚îÄ 01_data_exploration.ipynb      # Notebook kh√°m ph√° d·ªØ li·ªáu
    ‚îú‚îÄ‚îÄ 02_preprocessing.ipynb         # Notebook ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
    ‚îî‚îÄ‚îÄ 03_modeling.ipynb              # Notebook hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh
```

### 8.1. Gi·∫£i th√≠ch ch·ª©c nƒÉng t·ª´ng file/folder

#### `data/raw/`
- **Ch·ª©c nƒÉng**: Ch·ª©a d·ªØ li·ªáu g·ªëc t·ª´ dataset Kaggle
- **File**: `creditcard.csv` - Dataset g·ªëc v·ªõi 284,807 giao d·ªãch v√† 31 c·ªôt

#### `data/processed/`
- **Ch·ª©c nƒÉng**: Ch·ª©a d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω qua c√°c b∆∞·ªõc preprocessing
- **Files**:
  - `header.npy`: L∆∞u t√™n c√°c features (d√πng `np.save` v·ªõi `allow_pickle=True`)
  - `X_*_filled.npy`: C√°c phi√™n b·∫£n d·ªØ li·ªáu v·ªõi c√°c ph∆∞∆°ng ph√°p ƒëi·ªÅn missing values kh√°c nhau (demo)
  - `X_processed.npy`: D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c log transform v√† z-score standardization
  - `y.npy`: Vector labels (0 ho·∫∑c 1)
  - `X_train.npy`, `X_test.npy`: Features ƒë√£ ƒë∆∞·ª£c chia train/test
  - `y_train.npy`, `y_test.npy`: Labels ƒë√£ ƒë∆∞·ª£c chia train/test

#### `notebooks/01_data_exploration.ipynb`
- **Ch·ª©c nƒÉng**: Kh√°m ph√° v√† ph√¢n t√≠ch d·ªØ li·ªáu ban ƒë·∫ßu
- **N·ªôi dung ch√≠nh**:
  1. Load dataset b·∫±ng NumPy
  2. Ki·ªÉm tra missing values v√† th·ªëng k√™ m√¥ t·∫£
  3. Ph√¢n t√≠ch class distribution
  4. Ph√¢n t√≠ch c√°c features quan tr·ªçng (Time, Amount, V1-V28)
  5. Correlation analysis
  6. So s√°nh features gi·ªØa Normal v√† Fraud
  7. Feature engineering (rolling statistics)
  8. Statistical hypothesis testing
  9. X·ª≠ l√Ω missing values (demo)
  10. L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω

#### `notebooks/02_preprocessing.ipynb`
- **Ch·ª©c nƒÉng**: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu tr∆∞·ªõc khi modeling
- **N·ªôi dung ch√≠nh**:
  1. Load d·ªØ li·ªáu t·ª´ notebook 01
  2. Outlier detection (IQR v√† Z-score methods)
  3. Normalization & Standardization:
     - Min-Max Normalization
     - Z-score Standardization
     - Log Transformation
     - Decimal Scaling
  4. √Åp d·ª•ng preprocessing cu·ªëi c√πng (Log + Z-score)
  5. Train-Test Split (80-20)
  6. L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω

#### `notebooks/03_modeling.ipynb`
- **Ch·ª©c nƒÉng**: Hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh Logistic Regression
- **N·ªôi dung ch√≠nh**:
  1. Load d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω t·ª´ notebook 02
  2. Implement c√°c evaluation metrics (Accuracy, Precision, Recall, F1, AUC)
  3. Implement Logistic Regression class t·ª´ ƒë·∫ßu
  4. Training m√¥ h√¨nh v·ªõi Gradient Descent
  5. Visualize training loss history
  6. Evaluation tr√™n test set
  7. V·∫Ω Confusion Matrix
  8. V·∫Ω ROC Curve v√† t√≠nh AUC
  9. Ph√¢n t√≠ch v√† ƒë√°nh gi√° k·∫øt qu·∫£

#### `requirements.txt`
- **Ch·ª©c nƒÉng**: Li·ªát k√™ c√°c th∆∞ vi·ªán Python c·∫ßn thi·∫øt
- **N·ªôi dung**:
  ```
  numpy>=1.21.0
  matplotlib>=3.5.0
  seaborn>=0.11.0
  ```

#### `README.md`
- **Ch·ª©c nƒÉng**: T√†i li·ªáu h∆∞·ªõng d·∫´n chi ti·∫øt v·ªÅ d·ª± √°n
- **N·ªôi dung**: M√¥ t·∫£ ƒë·∫ßy ƒë·ªß v·ªÅ project, dataset, methods, results, v√† h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng

---

## 9. Challenges & Solutions

### 9.1. Kh√≥ khƒÉn g·∫∑p ph·∫£i khi d√πng NumPy

#### 9.1.1. Challenge: Load CSV file kh√¥ng d√πng Pandas

**V·∫•n ƒë·ªÅ**:
- NumPy kh√¥ng c√≥ h√†m ƒë·ªçc CSV tr·ª±c ti·∫øp nh∆∞ Pandas (`pd.read_csv()`)
- C·∫ßn parse header v√† convert data types th·ªß c√¥ng
- File CSV c√≥ header v·ªõi d·∫•u ngo·∫∑c k√©p (`"Time"`, `"V1"`, ...)

**Solution**:
```python
# ƒê·ªçc file CSV b·∫±ng np.genfromtxt v·ªõi dtype=str ƒë·ªÉ gi·ªØ nguy√™n format
data_str = np.genfromtxt(file_path, dtype=str, delimiter=',')

# X·ª≠ l√Ω header: Lo·∫°i b·ªè d·∫•u ngo·∫∑c k√©p
data_str = np.char.strip(data_str, '"')
header = data_str[0]
data_str = data_str[1:]

# Convert sang float64
data = data_str.astype(np.float64)
```

**B√†i h·ªçc**: NumPy c√≥ `np.char` module ƒë·ªÉ x·ª≠ l√Ω string arrays, v√† `np.genfromtxt()` c√≥ th·ªÉ ƒë·ªçc CSV nh∆∞ng c·∫ßn x·ª≠ l√Ω th√™m.

#### 9.1.2. Challenge: Vectorization thay v√¨ for loops

**V·∫•n ƒë·ªÅ**:
- Ban ƒë·∫ßu c√≥ th·ªÉ mu·ªën d√πng for loops ƒë·ªÉ x·ª≠ l√Ω t·ª´ng feature
- For loops ch·∫≠m v·ªõi dataset l·ªõn (284,807 samples)
- C·∫ßn t√≠nh to√°n th·ªëng k√™ cho nhi·ªÅu features

**Solution - S·ª≠ d·ª•ng Broadcasting v√† Vectorization**:

**V√≠ d·ª• 1: T√≠nh mean cho t·∫•t c·∫£ features**
```python
# ‚ùå Ch·∫≠m - D√πng for loop
means = np.zeros(n_features)
for i in range(n_features):
    means[i] = np.mean(data[:, i])

# ‚úÖ Nhanh - Vectorized
means = np.mean(data, axis=0)  # T√≠nh mean theo axis=0 (columns)
```

**V√≠ d·ª• 2: T√≠nh Z-score cho t·∫•t c·∫£ features**
```python
# ‚ùå Ch·∫≠m
z_scores = np.zeros_like(data)
for i in range(n_features):
    mean = np.mean(data[:, i])
    std = np.std(data[:, i])
    z_scores[:, i] = (data[:, i] - mean) / std

# ‚úÖ Nhanh - Broadcasting
mean_vals = np.mean(data, axis=0, keepdims=True)  # Shape: (1, n_features)
std_vals = np.std(data, axis=0, keepdims=True)    # Shape: (1, n_features)
z_scores = (data - mean_vals) / std_vals  # Broadcasting: (n_samples, n_features)
```

**V√≠ d·ª• 3: Fancy indexing thay v√¨ loop + if**
```python
# ‚ùå Ch·∫≠m
fraud_data = []
for i in range(len(y)):
    if y[i] == 1:
        fraud_data.append(X[i])

# ‚úÖ Nhanh - Boolean indexing
fraud_mask = (y == 1)
fraud_data = X[fraud_mask]
```

**B√†i h·ªçc**: Lu√¥n nghƒ© v·ªÅ c√°ch vectorize operations, s·ª≠ d·ª•ng broadcasting v√† fancy indexing.

#### 9.1.3. Challenge: T√≠nh to√°n distance matrix cho KNN (n·∫øu c√≥)

**V·∫•n ƒë·ªÅ**:
- C·∫ßn t√≠nh distance gi·ªØa m·ªói test point v√† t·∫•t c·∫£ train points
- For loops s·∫Ω r·∫•t ch·∫≠m v·ªõi dataset l·ªõn
- Memory c√≥ th·ªÉ kh√¥ng ƒë·ªß n·∫øu t√≠nh to√†n b·ªô distance matrix

**Solution - Broadcasting ƒë·ªÉ t√≠nh distance matrix**:
```python
# T√≠nh Euclidean distance gi·ªØa X1 (n1 samples) v√† X2 (n2 samples)
# K·∫øt qu·∫£: distance matrix shape (n1, n2)

# C√°ch 1: Broadcasting v·ªõi np.newaxis
diff = X1[:, np.newaxis, :] - X2[np.newaxis, :, :]  # Shape: (n1, n2, n_features)
distances = np.sqrt(np.sum(diff ** 2, axis=2))  # Shape: (n1, n2)

# C√°ch 2: S·ª≠ d·ª•ng c√¥ng th·ª©c vectorized
# d^2 = ||x1||^2 + ||x2||^2 - 2*x1*x2
X1_squared = np.sum(X1 ** 2, axis=1, keepdims=True)  # (n1, 1)
X2_squared = np.sum(X2 ** 2, axis=1)  # (n2,)
dot_product = X1 @ X2.T  # (n1, n2)
distances_squared = X1_squared + X2_squared - 2 * dot_product
distances = np.sqrt(np.maximum(distances_squared, 0))  # Tr√°nh negative do floating point
```

**B√†i h·ªçc**: Broadcasting l√† c√¥ng c·ª• m·∫°nh m·∫Ω ƒë·ªÉ t√≠nh to√°n hi·ªáu qu·∫£, nh∆∞ng c·∫ßn ch√∫ √Ω memory v·ªõi dataset l·ªõn.

#### 9.1.4. Challenge: Numerical stability trong sigmoid

**V·∫•n ƒë·ªÅ**:
- `exp(-z)` c√≥ th·ªÉ overflow khi z r·∫•t √¢m (z << 0)
- `exp(z)` c√≥ th·ªÉ overflow khi z r·∫•t d∆∞∆°ng (z >> 0)
- D·∫´n ƒë·∫øn `sigmoid(z)` tr·∫£ v·ªÅ `nan` ho·∫∑c `inf`

**Solution - Clip z values**:
```python
def _sigmoid(self, z):
    # Clip z ƒë·ªÉ tr√°nh overflow
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))
```

**Gi·∫£i th√≠ch**:
- `exp(-500) ‚âà 0` ‚Üí `sigmoid(-500) ‚âà 0`
- `exp(500) ‚âà inf` ‚Üí `sigmoid(500) ‚âà 1`
- V·ªõi z trong [-500, 500], sigmoid ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh

**B√†i h·ªçc**: Lu√¥n ch√∫ √Ω ƒë·∫øn numerical stability, ƒë·∫∑c bi·ªát v·ªõi c√°c h√†m exponential.

#### 9.1.5. Challenge: Underflow trong Naive Bayes (n·∫øu c√≥)

**V·∫•n ƒë·ªÅ**:
- Nh√¢n nhi·ªÅu probabilities nh·ªè c√≥ th·ªÉ g√¢y underflow
- `P(x|y) = P(x1|y) * P(x2|y) * ... * P(xn|y)` c√≥ th·ªÉ r·∫•t nh·ªè (g·∫ßn 0)
- D·∫´n ƒë·∫øn m·∫•t ƒë·ªô ch√≠nh x√°c s·ªë h·ªçc

**Solution - S·ª≠ d·ª•ng log probabilities**:
```python
# ‚ùå C√≥ th·ªÉ underflow
likelihood = np.prod(probabilities, axis=1)  # Nh√¢n nhi·ªÅu s·ªë nh·ªè

# ‚úÖ Tr√°nh underflow b·∫±ng log space
log_likelihood = np.sum(np.log(probabilities + 1e-15), axis=1)  # C·ªông log = nh√¢n
# Sau ƒë√≥ so s√°nh log probabilities thay v√¨ probabilities
```

**B√†i h·ªçc**: S·ª≠ d·ª•ng log space khi l√†m vi·ªác v·ªõi probabilities nh·ªè ƒë·ªÉ tr√°nh underflow.

#### 9.1.6. Challenge: T√≠nh to√°n th·ªëng k√™ ph·ª©c t·∫°p (Skewness, Kurtosis)

**V·∫•n ƒë·ªÅ**:
- NumPy kh√¥ng c√≥ h√†m `skew()` v√† `kurtosis()` s·∫µn (ho·∫∑c c√≥ nh∆∞ng c·∫ßn scipy)
- C·∫ßn implement t·ª´ ƒë·∫ßu b·∫±ng c√¥ng th·ª©c to√°n h·ªçc

**Solution - Implement t·ª´ c√¥ng th·ª©c**:
```python
def calculate_skewness(data):
    mean = np.mean(data, axis=0, keepdims=True)
    std = np.std(data, axis=0, ddof=1, keepdims=True)
    std = np.where(std == 0, 1, std)  # Tr√°nh chia cho 0
    centered = data - mean
    skew = np.mean((centered / std) ** 3, axis=0)
    return skew

def calculate_kurtosis(data):
    mean = np.mean(data, axis=0, keepdims=True)
    std = np.std(data, axis=0, ddof=1, keepdims=True)
    std = np.where(std == 0, 1, std)
    centered = data - mean
    kurt = np.mean((centered / std) ** 4, axis=0) - 3  # Excess kurtosis
    return kurt
```

**B√†i h·ªçc**: Hi·ªÉu r√µ c√¥ng th·ª©c to√°n h·ªçc gi√∫p implement c√°c h√†m kh√¥ng c√≥ s·∫µn.

#### 9.1.7. Challenge: X·ª≠ l√Ω division by zero

**V·∫•n ƒë·ªÅ**:
- Khi t√≠nh Z-score, n·∫øu std = 0 (feature kh√¥ng ƒë·ªïi), s·∫Ω g√¢y l·ªói division by zero
- Khi t√≠nh c√°c metrics, n·∫øu denominator = 0, s·∫Ω g√¢y l·ªói

**Solution - S·ª≠ d·ª•ng np.where ƒë·ªÉ x·ª≠ l√Ω edge cases**:
```python
# V√≠ d·ª• 1: Z-score standardization
std_vals = np.std(data, axis=0, ddof=1, keepdims=True)
std_vals = np.where(std_vals == 0, 1, std_vals)  # Thay 0 b·∫±ng 1
z_scores = (data - mean_vals) / std_vals

# V√≠ d·ª• 2: Precision score
if tp + fp == 0:
    return 0.0  # Kh√¥ng c√≥ positive predictions
return tp / (tp + fp)
```

**B√†i h·ªçc**: Lu√¥n ki·ªÉm tra edge cases v√† x·ª≠ l√Ω division by zero.

### 9.2. C√°ch gi·∫£i quy·∫øt

#### 9.2.1. ƒê·ªçc t√†i li·ªáu NumPy

- **Broadcasting**: Hi·ªÉu r√µ c√°ch NumPy broadcast arrays
- **Fancy indexing**: S·ª≠ d·ª•ng boolean masks v√† integer arrays
- **Universal functions (ufuncs)**: T·∫≠n d·ª•ng c√°c h√†m vectorized c·ªßa NumPy
- **Memory efficiency**: S·ª≠ d·ª•ng views thay v√¨ copies khi c√≥ th·ªÉ

#### 9.2.2. Vectorization mindset

- **Lu√¥n nghƒ© v·ªÅ c√°ch vectorize**: Tr∆∞·ªõc khi vi·∫øt for loop, nghƒ© xem c√≥ th·ªÉ vectorize kh√¥ng
- **Broadcasting**: S·ª≠ d·ª•ng `np.newaxis` v√† `keepdims=True` ƒë·ªÉ control shape
- **Fancy indexing**: S·ª≠ d·ª•ng boolean masks thay v√¨ loops + if

#### 9.2.3. Numerical stability

- **Overflow/Underflow**: Ch√∫ √Ω ƒë·∫øn c√°c h√†m exponential, log
- **Log space**: S·ª≠ d·ª•ng log probabilities khi l√†m vi·ªác v·ªõi probabilities nh·ªè
- **Clipping**: Clip values ƒë·ªÉ tr√°nh overflow
- **Epsilon**: Th√™m epsilon nh·ªè (1e-15) khi t√≠nh log ƒë·ªÉ tr√°nh log(0)

#### 9.2.4. Memory efficiency

- **Views vs Copies**: S·ª≠ d·ª•ng views (`data[mask]`) thay v√¨ copies khi c√≥ th·ªÉ
- **In-place operations**: S·ª≠ d·ª•ng `+=`, `-=` thay v√¨ `= +` khi c√≥ th·ªÉ
- **Memory mapping**: V·ªõi dataset r·∫•t l·ªõn, c√≥ th·ªÉ d√πng `np.memmap()`

#### 9.2.5. Testing v√† Debugging

- **Test t·ª´ng function nh·ªè**: Test t·ª´ng function tr∆∞·ªõc khi t√≠ch h·ª£p
- **Ki·ªÉm tra shapes**: Lu√¥n ki·ªÉm tra shape c·ªßa arrays
- **Visualize intermediate results**: In ra m·ªôt v√†i gi√° tr·ªã ƒë·ªÉ ki·ªÉm tra
- **Compare v·ªõi reference**: So s√°nh k·∫øt qu·∫£ v·ªõi scikit-learn ho·∫∑c Pandas (n·∫øu c√≥ th·ªÉ)

#### 9.2.6. Performance optimization

- **Profile code**: S·ª≠ d·ª•ng `%timeit` trong Jupyter ƒë·ªÉ ƒëo th·ªùi gian
- **Avoid unnecessary copies**: S·ª≠ d·ª•ng views khi c√≥ th·ªÉ
- **Use appropriate dtypes**: S·ª≠ d·ª•ng `float32` thay v√¨ `float64` n·∫øu ƒë·ªß ƒë·ªô ch√≠nh x√°c
- **Batch processing**: X·ª≠ l√Ω theo batch n·∫øu dataset qu√° l·ªõn

---

## 10. Future Improvements

### 10.1. X·ª≠ l√Ω Class Imbalance

#### 10.1.1. Class Weighting

**√ù t∆∞·ªüng**: TƒÉng tr·ªçng s·ªë cho class thi·ªÉu s·ªë trong loss function

**Implementation**:
```python
# Trong Binary Cross-Entropy Loss
class_weight_0 = len(y) / (2 * np.sum(y == 0))  # Weight cho class 0
class_weight_1 = len(y) / (2 * np.sum(y == 1))  # Weight cho class 1

# Weighted loss
loss = -np.mean(class_weight_0 * (1-y) * np.log(1-y_pred + 1e-15) + 
                class_weight_1 * y * np.log(y_pred + 1e-15))
```

**L·ª£i √≠ch**: M√¥ h√¨nh s·∫Ω ch√∫ √Ω nhi·ªÅu h∆°n ƒë·∫øn class thi·ªÉu s·ªë (Fraud)

#### 10.1.2. SMOTE (Synthetic Minority Oversampling Technique)

**√ù t∆∞·ªüng**: T·∫°o synthetic samples cho class thi·ªÉu s·ªë

**C√°ch ho·∫°t ƒë·ªông**:
1. Ch·ªçn m·ªôt sample t·ª´ class thi·ªÉu s·ªë
2. T√¨m k nearest neighbors t·ª´ c√πng class
3. T·∫°o synthetic sample b·∫±ng c√°ch interpolate gi·ªØa sample v√† neighbors

**L·ª£i √≠ch**: TƒÉng s·ªë l∆∞·ª£ng samples c·ªßa class thi·ªÉu s·ªë m√† kh√¥ng ch·ªâ duplicate

#### 10.1.3. Undersampling

**√ù t∆∞·ªüng**: Gi·∫£m s·ªë l∆∞·ª£ng samples c·ªßa class ƒëa s·ªë

**Ph∆∞∆°ng ph√°p**:
- Random undersampling
- Tomek Links
- Edited Nearest Neighbors

**L∆∞u √Ω**: C·∫ßn c·∫©n th·∫≠n ƒë·ªÉ kh√¥ng m·∫•t th√¥ng tin quan tr·ªçng

### 10.2. Feature Engineering

#### 10.2.1. Time-based Features

**T·∫°o features t·ª´ Time**:
```python
# Hour of day
hours = (time_data // 3600) % 24

# Day of week (n·∫øu c√≥ ƒë·ªß d·ªØ li·ªáu)
days = (time_data // 86400) % 7

# Is weekend
is_weekend = (days == 5) | (days == 6)

# Is night (2-6 AM)
is_night = (hours >= 2) & (hours < 6)
```

**L·ª£i √≠ch**: T·∫≠n d·ª•ng pattern th·ªùi gian ƒë√£ ph√°t hi·ªán (fraud rate cao v√†o ban ƒë√™m)

#### 10.2.2. Amount Binning

**Chia Amount th√†nh c√°c bins**:
```python
# T·∫°o bins d·ª±a tr√™n quantiles
bins = np.percentile(amount_data, [0, 25, 50, 75, 100])
amount_binned = np.digitize(amount_data, bins)
```

**L·ª£i √≠ch**: Gi·∫£m ·∫£nh h∆∞·ªüng c·ªßa outliers, t·∫°o features categorical

#### 10.2.3. Interaction Features

**T·∫°o features t∆∞∆°ng t√°c gi·ªØa c√°c features quan tr·ªçng**:
```python
# V√≠ d·ª•: T∆∞∆°ng t√°c gi·ªØa V3 v√† V14 (2 features quan tr·ªçng nh·∫•t)
interaction = V3 * V14

# Ho·∫∑c ratio
ratio = V3 / (V14 + 1e-10)  # Tr√°nh chia cho 0
```

**L·ª£i √≠ch**: N·∫Øm b·∫Øt m·ªëi quan h·ªá phi tuy·∫øn gi·ªØa c√°c features

#### 10.2.4. Polynomial Features

**T·∫°o polynomial features**:
```python
# B·∫≠c 2
X_poly = np.column_stack([X, X**2])

# Ho·∫∑c ch·ªâ cho m·ªôt s·ªë features quan tr·ªçng
important_features = X[:, [v3_idx, v14_idx, v17_idx]]
X_poly = np.column_stack([X, important_features**2])
```

**L·ª£i √≠ch**: N·∫Øm b·∫Øt m·ªëi quan h·ªá phi tuy·∫øn

### 10.3. Model Improvements

#### 10.3.1. Hyperparameter Tuning

**Tune c√°c hyperparameters**:
- Learning rate: Th·ª≠ 0.001, 0.01, 0.1
- Max iterations: TƒÉng l√™n n·∫øu ch∆∞a converge
- Regularization: Th√™m L1/L2 regularization ƒë·ªÉ tr√°nh overfitting

**Implementation L2 Regularization**:
```python
# Th√™m v√†o loss function
L2_penalty = lambda_reg * np.sum(self.weights ** 2)
loss = binary_cross_entropy_loss + L2_penalty

# Th√™m v√†o gradient
dw = (1/m) * X.T @ (y_pred - y) + 2 * lambda_reg * self.weights
```

#### 10.3.2. Threshold Optimization

**Tune threshold ƒë·ªÉ c√¢n b·∫±ng Precision v√† Recall**:
```python
# Th·ª≠ c√°c threshold kh√°c nhau
thresholds = np.arange(0.1, 0.9, 0.05)
best_f1 = 0
best_threshold = 0.5

for threshold in thresholds:
    y_pred = (y_pred_proba >= threshold).astype(int)
    f1 = f1_score(y_test, y_pred)
    if f1 > best_f1:
        best_f1 = f1
        best_threshold = threshold
```

**L·ª£i √≠ch**: C√≥ th·ªÉ tƒÉng Recall m√† kh√¥ng gi·∫£m Precision qu√° nhi·ªÅu

#### 10.3.3. Ensemble Methods

**K·∫øt h·ª£p nhi·ªÅu m√¥ h√¨nh**:
- **Voting**: K·∫øt h·ª£p predictions t·ª´ nhi·ªÅu m√¥ h√¨nh
- **Stacking**: D√πng m√¥ h√¨nh kh√°c ƒë·ªÉ combine predictions
- **Bagging**: Train nhi·ªÅu m√¥ h√¨nh tr√™n c√°c subsets kh√°c nhau

#### 10.3.4. Advanced Algorithms

**Th·ª≠ c√°c thu·∫≠t to√°n kh√°c** (implement t·ª´ ƒë·∫ßu b·∫±ng NumPy):
- **Decision Trees**: C√≥ th·ªÉ x·ª≠ l√Ω t·ªët v·ªõi imbalanced data
- **Random Forest**: Ensemble c·ªßa Decision Trees
- **Neural Networks**: N·∫øu ƒë∆∞·ª£c ph√©p, c√≥ th·ªÉ th·ª≠ MLP ƒë∆°n gi·∫£n

### 10.4. Evaluation Improvements

#### 10.4.1. Precision-Recall Curve

**V·∫Ω PR Curve thay v√¨ ch·ªâ ROC Curve**:
- PR Curve t·ªët h∆°n ROC Curve cho imbalanced data
- Focus v√†o Precision v√† Recall thay v√¨ FPR

#### 10.4.2. Cost-Sensitive Evaluation

**ƒê√°nh gi√° d·ª±a tr√™n cost matrix**:
```python
# Cost matrix
cost_matrix = {
    'TN': 0,      # True Negative: Kh√¥ng c√≥ cost
    'FP': 10,     # False Positive: L√†m phi·ªÅn kh√°ch h√†ng
    'FN': 1000,   # False Negative: M·∫•t ti·ªÅn do gian l·∫≠n
    'TP': -100    # True Positive: Ph√°t hi·ªán ƒë∆∞·ª£c, ti·∫øt ki·ªám ti·ªÅn
}

# T√≠nh total cost
total_cost = (TN * cost_matrix['TN'] + 
              FP * cost_matrix['FP'] + 
              FN * cost_matrix['FN'] + 
              TP * cost_matrix['TP'])
```

**L·ª£i √≠ch**: Ph·∫£n √°nh ƒë√∫ng t√°c ƒë·ªông th·ª±c t·∫ø c·ªßa c√°c lo·∫°i l·ªói

#### 10.4.3. Cross-Validation

**S·ª≠ d·ª•ng k-fold cross-validation**:
```python
def k_fold_cross_validation(X, y, k=5):
    n_samples = len(X)
    fold_size = n_samples // k
    scores = []
    
    for i in range(k):
        # Split data
        val_start = i * fold_size
        val_end = (i + 1) * fold_size
        
        X_val = X[val_start:val_end]
        y_val = y[val_start:val_end]
        X_train = np.concatenate([X[:val_start], X[val_end:]])
        y_train = np.concatenate([y[:val_start], y[val_end:]])
        
        # Train and evaluate
        model.fit(X_train, y_train)
        score = model.evaluate(X_val, y_val)
        scores.append(score)
    
    return np.mean(scores), np.std(scores)
```

**L·ª£i √≠ch**: ƒê√°nh gi√° ·ªïn ƒë·ªãnh h∆°n, kh√¥ng ph·ª• thu·ªôc v√†o m·ªôt l·∫ßn split

### 10.5. Code Optimization

#### 10.5.1. Memory Optimization

**S·ª≠ d·ª•ng memory mapping cho dataset l·ªõn**:
```python
# Thay v√¨ load to√†n b·ªô v√†o memory
X = np.load('X_train.npy')

# S·ª≠ d·ª•ng memory mapping
X = np.load('X_train.npy', mmap_mode='r')  # Read-only memory map
```

**L·ª£i √≠ch**: Ti·∫øt ki·ªám memory, c√≥ th·ªÉ x·ª≠ l√Ω dataset l·ªõn h∆°n

#### 10.5.2. Parallel Processing

**S·ª≠ d·ª•ng multiprocessing cho cross-validation**:
```python
from multiprocessing import Pool

def train_fold(args):
    X_train, y_train, X_val, y_val = args
    model = LogisticRegression()
    model.fit(X_train, y_train)
    return model.evaluate(X_val, y_val)

# Parallel processing
with Pool(processes=4) as pool:
    scores = pool.map(train_fold, fold_args)
```

**L·ª£i √≠ch**: TƒÉng t·ªëc ƒë·ªô training khi c√≥ nhi·ªÅu CPU cores

#### 10.5.3. Caching

**Cache c√°c k·∫øt qu·∫£ t√≠nh to√°n trung gian**:
```python
import pickle

# Cache preprocessed data
if os.path.exists('X_processed_cache.npy'):
    X_processed = np.load('X_processed_cache.npy')
else:
    X_processed = preprocess(X)
    np.save('X_processed_cache.npy', X_processed)
```

**L·ª£i √≠ch**: Tr√°nh t√≠nh to√°n l·∫°i c√°c k·∫øt qu·∫£ ƒë√£ c√≥

### 10.6. Documentation v√† Code Quality

#### 10.6.1. Refactor th√†nh modules

**T√°ch code th√†nh c√°c modules**:
```
src/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ data_processing.py    # C√°c h√†m x·ª≠ l√Ω d·ªØ li·ªáu
‚îú‚îÄ‚îÄ models.py             # C√°c m√¥ h√¨nh ML
‚îú‚îÄ‚îÄ metrics.py            # C√°c evaluation metrics
‚îî‚îÄ‚îÄ visualization.py     # C√°c h√†m visualization
```

**L·ª£i √≠ch**: Code d·ªÖ maintain v√† reuse h∆°n

#### 10.6.2. Unit Tests

**Vi·∫øt unit tests cho c√°c functions**:
```python
def test_precision_score():
    y_true = np.array([0, 1, 1, 0, 1])
    y_pred = np.array([0, 1, 0, 0, 1])
    assert precision_score(y_true, y_pred) == 1.0
```

**L·ª£i √≠ch**: ƒê·∫£m b·∫£o code ho·∫°t ƒë·ªông ƒë√∫ng

#### 10.6.3. Type Hints v√† Docstrings

**Th√™m type hints v√† docstrings**:
```python
def precision_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Calculate precision score.
    
    Parameters:
    -----------
    y_true : np.ndarray
        True labels
    y_pred : np.ndarray
        Predicted labels
    
    Returns:
    --------
    float
        Precision score
    """
    ...
```

**L·ª£i √≠ch**: Code d·ªÖ ƒë·ªçc v√† maintain h∆°n

---

## 11. Contributors

### 11.1. Th√¥ng tin t√°c gi·∫£

- **T√™n**: [T√™n sinh vi√™n]
- **MSSV**: 23127516
- **Email**: [Email]
- **Tr∆∞·ªùng**: Tr∆∞·ªùng ƒê·∫°i h·ªçc Khoa h·ªçc T·ª± nhi√™n, ƒê·∫°i h·ªçc Qu·ªëc gia TP.HCM
- **Khoa**: Khoa C√¥ng ngh·ªá Th√¥ng tin
- **B·ªô m√¥n**: Khoa h·ªçc M√°y t√≠nh
- **M√¥n h·ªçc**: Programming for Data Science

### 11.2. Contact

N·∫øu c√≥ c√¢u h·ªèi, g√≥p √Ω ho·∫∑c mu·ªën ƒë√≥ng g√≥p cho d·ª± √°n, vui l√≤ng li√™n h·ªá:

- **Email**: [Email]
- **GitHub**: [GitHub username] (n·∫øu c√≥)
- **LinkedIn**: [LinkedIn profile] (n·∫øu c√≥)

### 11.3. Acknowledgments

- **Dataset**: C·∫£m ∆°n ULB Machine Learning Group v√† Kaggle ƒë√£ cung c·∫•p dataset
- **Gi·∫£ng vi√™n**: C·∫£m ∆°n gi·∫£ng vi√™n m√¥n Programming for Data Science ƒë√£ h∆∞·ªõng d·∫´n
- **T√†i li·ªáu**: C·∫£m ∆°n c·ªông ƒë·ªìng NumPy, Matplotlib, Seaborn ƒë√£ cung c·∫•p t√†i li·ªáu tuy·ªát v·ªùi

---

## 12. License

This project is licensed under the **MIT License** - see the LICENSE file for details.

**MIT License** cho ph√©p:
- ‚úÖ S·ª≠ d·ª•ng th∆∞∆°ng m·∫°i
- ‚úÖ S·ª≠ d·ª•ng c√° nh√¢n
- ‚úÖ S·ª≠a ƒë·ªïi
- ‚úÖ Ph√¢n ph·ªëi
- ‚úÖ Sublicense

**Y√™u c·∫ßu**:
- ‚ö†Ô∏è Bao g·ªìm license v√† copyright notice
- ‚ö†Ô∏è Kh√¥ng c√≥ warranty

---

## References

1. **NumPy Documentation**: https://numpy.org/doc/
2. **Credit Card Fraud Detection Dataset**: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud
3. **Matplotlib Documentation**: https://matplotlib.org/
4. **Seaborn Documentation**: https://seaborn.pydata.org/
5. **Logistic Regression Theory**: 
   - Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
   - Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springer.
6. **Imbalanced Data Handling**:
   - Chawla, N. V., et al. (2002). SMOTE: Synthetic Minority Over-sampling Technique. Journal of Artificial Intelligence Research.
7. **Evaluation Metrics for Imbalanced Data**:
   - Saito, T., & Rehmsmeier, M. (2015). The Precision-Recall Plot Is More Informative Than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets. PLOS ONE.

---

## L∆∞u √Ω

**D·ª± √°n n√†y ƒë∆∞·ª£c th·ª±c hi·ªán nh∆∞ m·ªôt ph·∫ßn c·ªßa b√†i t·∫≠p h·ªçc t·∫≠p**. T·∫•t c·∫£ c√°c thu·∫≠t to√°n Machine Learning ƒë·ªÅu ƒë∆∞·ª£c **implement t·ª´ ƒë·∫ßu b·∫±ng NumPy** ƒë·ªÉ h·ªçc h·ªèi v√† hi·ªÉu s√¢u v·ªÅ c√°ch ho·∫°t ƒë·ªông c·ªßa c√°c thu·∫≠t to√°n, kh√¥ng s·ª≠ d·ª•ng c√°c th∆∞ vi·ªán ML c√≥ s·∫µn nh∆∞ scikit-learn.

**M·ª•c ƒë√≠ch ch√≠nh**:
- ‚úÖ Hi·ªÉu r√µ c√°ch ho·∫°t ƒë·ªông c·ªßa c√°c thu·∫≠t to√°n ML c∆° b·∫£n
- ‚úÖ L√†m ch·ªß NumPy v√† vectorization
- ‚úÖ √Åp d·ª•ng ki·∫øn th·ª©c to√°n h·ªçc v√†o th·ª±c t·∫ø
- ‚úÖ X·ª≠ l√Ω b√†i to√°n imbalanced data

**Kh√¥ng ph·∫£i m·ª•c ƒë√≠ch**:
- ‚ùå T·∫°o ra m√¥ h√¨nh production-ready t·ªët nh·∫•t
- ‚ùå So s√°nh v·ªõi c√°c m√¥ h√¨nh state-of-the-art
- ‚ùå T·ªëi ∆∞u h√≥a performance c·ª±c ƒë·∫°i

---

**C·∫£m ∆°n b·∫°n ƒë√£ quan t√¢m ƒë·∫øn d·ª± √°n!** üôè
